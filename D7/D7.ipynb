{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"D7.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JyT_wikR2WaS"
   },
   "source": [
    "# Discussion 7: Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOulCecT2WaS"
   },
   "source": [
    "Support vector machines (SVMs) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
    "In this notebook, we will develop the intuition behind support vector machines and their use in classification problems.\n",
    "\n",
    "SVMs are powerful classification method for several reasons:\n",
    "\n",
    "- They rely on relatively few support vectors, making them compact and memory-efficient.\n",
    "- They offer fast prediction once the model is trained.\n",
    "- They work well with high-dimensional data, even when the number of dimensions exceeds the number of samples.\n",
    "- Their versatility is enhanced by their integration with kernel methods, enabling them to adapt to various data types.\n",
    "\n",
    "However, SVMs also have some drawbacks:\n",
    "\n",
    "- The computational cost scales with the number of samples, which can be a limitation for large datasets.\n",
    "- The choice of the regularization parameter C significantly impacts the results and must be carefully selected via cross-validation, which can be expensive for larger datasets.\n",
    "- The results lack a direct probabilistic interpretation.\n",
    "\n",
    "\n",
    "## Instructions:\n",
    "\n",
    "Just because you pass the public tests **does not mean you will pass hidden tests**. Public tests are there to guide your implementation, but the correct answer is hidden.\n",
    "\n",
    "Do NOT change any of the given code.\n",
    "\n",
    "Read the markdown cells carefully, as they'll provide hints towards writing your solutions. For each question, you'll be expected to import the required packages and implement based upon the description. All necessary information will be provided, so again, read carefully!\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z4CxlzcE2WaT"
   },
   "source": [
    "## Motivation of Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rHnk4WUF2WaT"
   },
   "source": [
    "Consider the simple case of a classification task, in which the two classes of points are well separated:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kt9c1-MI2WaT",
    "outputId": "9994af04-6bf1-43da-c7aa-7b843e193114"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "# use seaborn plotting defaults\n",
    "import seaborn as sns; sns.set()\n",
    "from sklearn.datasets import make_blobs\n",
    "\n",
    "\n",
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=18, cluster_std=1)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K6zwdUOs2WaU"
   },
   "source": [
    "A linear discriminative classifier seeks to create a classification model by drawing a straight line that separates two sets of data. When dealing with two-dimensional data, such as the example shown here, this task can be done manually. However, a challenge arises because there are multiple possible dividing lines that can accurately discriminate between the two classes. We can illustrate this by drawing different lines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qKaitG-A2WaU",
    "outputId": "ad82bda2-0d31-45c1-fe89-e28cf1531991"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0, 10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Spectral')\n",
    "plt.plot([3], [-4], 'o', color='black', markeredgewidth=4, markersize=12)\n",
    "\n",
    "for m, b in [(1, -8), (0.4, -6), (-0.2, -2)]:\n",
    "    plt.plot(xfit, m * xfit + b, '-k')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WOpHzhIt2WaU"
   },
   "source": [
    "These three separators are remarkably distinct, yet they all effectively separate the samples. Depending on the decision boundary chosen, a new data point (indicated by the black point in the plot) will be classified differently. It is clear that relying solely on the intuition of \"drawing a line between classes\" is insufficient, and we must delve deeper to construct a classifier that will generalize best to unseen data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Dvp712oX2WaV"
   },
   "source": [
    "## Support Vector Machines: Maximizing the *Margin*\n",
    "\n",
    "Support vector machines offer one way to improve on this.\n",
    "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point.\n",
    "Here is an example of how this might look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z4PuH_PS2WaV",
    "outputId": "11048842-4588-46f3-a178-eb9b39c4e6c8"
   },
   "outputs": [],
   "source": [
    "xfit = np.linspace(0,10)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Spectral')\n",
    "plt.plot([3], [-4], 'o', color='black', markeredgewidth=4, markersize=12)\n",
    "d = 0.3\n",
    "for m, b in [(1, -8), (0.4, -6), (-0.2, -2)]:\n",
    "    yfit = m * xfit + b\n",
    "    plt.plot(xfit, yfit, '-k')\n",
    "    plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',\n",
    "                     color='#AAAAAA', alpha=0.8)\n",
    "\n",
    "plt.xlim(0, 10);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FUVh8TSh2WaV"
   },
   "source": [
    "### Question 1: Fitting a Support Vector Machine\n",
    "\n",
    "Support vector machines serve as a maximum margin estimator; the optimal model in support vector machines is determined by selecting the line that maximizes the margin.\n",
    "\n",
    "To get started, you will implement a linear Support Vector Classifier by importing https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html and fitting it on the entire `X` and `y` data generated above.\n",
    "\n",
    "\n",
    "In this section, try a very large regularization value of 1000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L3hfmFXk2WaW"
   },
   "source": [
    "We include a helper function to plot the SVM decision boundaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VqbytfTd2WaW"
   },
   "outputs": [],
   "source": [
    "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
    "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
    "    if ax is None:\n",
    "        ax = plt.gca()\n",
    "    xlim = ax.get_xlim()\n",
    "    ylim = ax.get_ylim()\n",
    "    \n",
    "    # create grid to evaluate model\n",
    "    x = np.linspace(xlim[0], xlim[1], 30)\n",
    "    y = np.linspace(ylim[0], ylim[1], 30)\n",
    "    Y, X = np.meshgrid(y, x)\n",
    "    xy = np.vstack([X.ravel(), Y.ravel()]).T\n",
    "    P = model.decision_function(xy).reshape(X.shape)\n",
    "    \n",
    "    # plot decision boundary and margins\n",
    "    ax.contour(X, Y, P, colors='k',\n",
    "               levels=[-1, 0, 1], alpha=0.5,\n",
    "               linestyles=['--', '-', '--'])\n",
    "    \n",
    "    # plot support vectors\n",
    "    if plot_support:\n",
    "        ax.scatter(model.support_vectors_[:, 0],\n",
    "                   model.support_vectors_[:, 1],\n",
    "                   s=300, linewidth=1, facecolors='none');\n",
    "    ax.set_xlim(xlim)\n",
    "    ax.set_ylim(ylim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TjI-ahv92WaV",
    "outputId": "304e1dcf-d696-4606-8b74-5204f1a4e095",
    "scrolled": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Necessary Packages\n",
    "...\n",
    "# Define and Fit your Model\n",
    "model = ...\n",
    "...\n",
    "\n",
    "# Plot the Data and the Decision Boundary\n",
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rv3lr2dH2WaW"
   },
   "source": [
    "This is the dividing line that maximizes the margin between the two sets of points.\n",
    "Notice that a few of the training points just touch the margin: they are indicated by the black circles in this figure.\n",
    "These points are the pivotal elements of this fit, and are known as the *support vectors*, and give the algorithm its name.\n",
    "In Scikit-Learn, the identity of these points are stored in the ``support_vectors_`` attribute of the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Fit SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "e-fqn99D2WaX"
   },
   "source": [
    "## Question 2: Support Vectors\n",
    "\n",
    "The success of this SVM classifier is attributed to the fact that, for the fitting process, only the position of the support vectors is significant. Points that are correctly classified and located further away from the margin do not alter the fit. This is due to the fact that these points do not contribute to the loss function utilized for fitting the model, and thus their position and quantity are irrelevant as long as they do not cross the margin.\n",
    "\n",
    "This phenomenon is evident when examining the models learned from the first 60 points and the first 120 points of the dataset.\n",
    "\n",
    "**Hint**: Your decision boundaries should look the same!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bnlSHLdE2WaX",
    "outputId": "2e61b08f-19dd-4c61-9c4a-11de4e6e6d15",
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_svm(N=10, ax=None):\n",
    "    X, y = make_blobs(n_samples=200, centers=2,\n",
    "                      random_state=0, cluster_std=0.70)\n",
    "    # Select N points\n",
    "    X = ...\n",
    "    y = ...\n",
    "    \n",
    "    # Fit an SVC again with a large value of C\n",
    "    model = ...\n",
    "    ...\n",
    "    \n",
    "    # Plot the data using ax.scatter\n",
    "    if ax:\n",
    "        ...\n",
    "    \n",
    "        # Call the helper function defined about for plotting the decision boundary\n",
    "        ...\n",
    "    \n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "for axi, N in zip(ax, [60, 100]):\n",
    "    model = plot_svm(N, axi)\n",
    "    axi.set_title('N = {0}'.format(N))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Egk3fkVD2WaX"
   },
   "source": [
    "The model and support vectors for 60 training points are visible in the left panel; the right panel shows that we have increased the number of training points by two, but the model remains the same, and the three support vectors from the left panel are still the support vectors from the right panel. This ability to remain unaffected by distant points' behavior is one of the SVM model's advantages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Support Vectors\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iDzJUHI_2WaY"
   },
   "source": [
    "### Question 3: Kernel SVM\n",
    "\n",
    "SVM exhibits its potential when it is utilized beyond the linear case, with kernels. To illustrate the necessity of kernels, let's examine some data that cannot be separated linearly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "15Gt2NpC2WaY",
    "outputId": "6c29b225-3eb6-4cf5-cc27-c4505a91eca6"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles\n",
    "X, y = make_circles(100, factor=.1, noise=.1, random_state=42)\n",
    "\n",
    "model = SVC(kernel='linear')\n",
    "model.fit(X, y)\n",
    "\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Spectral')\n",
    "plot_svc_decision_function(model, plot_support=False);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VLOI4kgl2WaY"
   },
   "source": [
    "It's apparent that linear discrimination can never segregate this data. However, we can elevate the data to a higher dimension, making a linear separator feasible. For instance, a straightforward projection we can employ is calculating a radial basis function, centered on the central cluster:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CnQYm_tY2WaY"
   },
   "outputs": [],
   "source": [
    "r = np.exp(-(X ** 2).sum(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xf-SuXTP2WaZ",
    "outputId": "473bdfb6-2302-4f72-edee-7be91af5322f"
   },
   "outputs": [],
   "source": [
    "from mpl_toolkits import mplot3d\n",
    "\n",
    "def plot_3D(r, elev=30, azim=30, X=X, y=y):\n",
    "    ax = plt.subplot(projection='3d')\n",
    "    ax.scatter3D(X[:, 0], X[:, 1], r, c=y, s=50, cmap='autumn')\n",
    "    ax.view_init(elev=elev, azim=azim)\n",
    "    ax.set_xlabel('x')\n",
    "    ax.set_ylabel('y')\n",
    "    ax.set_zlabel('r')\n",
    "\n",
    "plot_3D(r, X=X, y=y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Bo0bwNob2WaZ"
   },
   "source": [
    "However, selecting and adjusting our projection requires careful consideration. If we do not correctly center our radial basis function, we cannot achieve such neat and linearly separable results. In general, this selection process poses a problem, and we need to find the optimal basis functions automatically.\n",
    "\n",
    "One strategy to this end is to compute a basis function centered at *every* point in the dataset, and let the SVM algorithm sift through the results.\n",
    "This type of basis function transformation is known as a *kernel transformation*, as it is based on a similarity relationship (or kernel) between each pair of points.\n",
    "\n",
    "However, projecting $N$ points into $N$ dimensions— might become very computationally intensive as $N$ grows large.\n",
    "However, because of a neat little procedure known as the [*kernel trick*](https://en.wikipedia.org/wiki/Kernel_trick), a fit on kernel-transformed data can be done implicitly—that is, without ever building the full $N$-dimensional representation of the kernel projection!\n",
    "This kernel trick is built into the SVM, and is one of the reasons the method is so powerful.\n",
    "\n",
    "Implement a kernelized SVM by swapping the linear kernel for an RBF (radial basis function) kernel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Type your answer here, replacing this text._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h8_h-MaB2Waa",
    "outputId": "317de468-64ad-4a74-9c8c-362b3be3f7ac",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Fit an SVM with an RBF kernel and C=1000\n",
    "model = ...\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ca2hc8vt2Wab",
    "outputId": "58bc3275-4512-4eef-c5e5-abadc9a7e49d",
    "tags": []
   },
   "outputs": [],
   "source": [
    "...\n",
    "..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J1pCORjH2Wac"
   },
   "source": [
    "By employing this kernelized support vector machine, we can develop an appropriate nonlinear decision boundary. This kernel transformation approach is frequently used in machine learning to convert rapid linear methods into efficient nonlinear methods, primarily for models that can employ the kernel trick."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Kernel SVM\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JBrkXns2Wac"
   },
   "source": [
    "### Question 4: Margins\n",
    "\n",
    "In perfectly clean, simulated datasets a perfect decision boundary exists. But what about noisier data that you might encounter in the real world?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8Zy2wj2d2Wac",
    "outputId": "06e29d20-399e-4990-bcb9-d71540a2ff78"
   },
   "outputs": [],
   "source": [
    "X, y = make_blobs(n_samples=100, centers=2,\n",
    "                  random_state=0, cluster_std=1.2)\n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='Spectral');"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pk0AMhTN2Wac"
   },
   "source": [
    "In the SVM implementation, the hardness of the margin is controlled by a regularization hyperparameter, most often known as $C$.\n",
    "\n",
    "Large values of $C$ construct SVM with a hard margin. Points can not lie within the margin.\n",
    "Small values of $C$, construct SVM with a soft margin. Points can lie wiwthin the margin.\n",
    "\n",
    "Split your data, `0.8` in train, `0.2` in test, `random_state=42`. Train two separate models using the given values of `C` $10.0$ and $0.1$. Append your test set performance to the `test_accuracies` list.\n",
    "Generate two plots demonstrating this characteristic of SVMs.\n",
    "\n",
    "You may use the `train_test_split` and `accuracy_score` functions from sklearn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Tt2gWZWN2Wac",
    "outputId": "1f6285cd-24b4-4288-c5e7-b9d921dd59d3",
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Import Packages\n",
    "...\n",
    "...\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
    "fig.subplots_adjust(left=0.0625, right=0.95, wspace=0.1)\n",
    "\n",
    "\n",
    "...\n",
    "test_acc = ...\n",
    "\n",
    "for axi, C in zip(ax, [10.0, 0.1]):\n",
    "    # Define your model\n",
    "    model = ...\n",
    "    \n",
    "    # Calculate and append Accuracy\n",
    "    y_pred = ...\n",
    "    accuracy = ...\n",
    "    ...\n",
    "    \n",
    "    # Plotting\n",
    "    ...\n",
    "    ...\n",
    "    axi.set_title('C = {0:.1f}'.format(C), size=14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"Soft Margins\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9_3DsqdG2Wad"
   },
   "source": [
    "The optimal value of the $C$ parameter will depend on your dataset, and should be tuned using cross-validation or a similar procedure (refer back to [Hyperparameters and Model Validation](05.03-Hyperparameters-and-Model-Validation.ipynb))."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "Fit SVM": {
     "name": "Fit SVM",
     "points": 0.5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> \n>>> assert model.fit_status_ == 0, \"Checks that you fit your model\"\n>>> assert isinstance(model, sklearn.svm.SVC)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Kernel SVM": {
     "name": "Kernel SVM",
     "points": 0.5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Soft Margins": {
     "name": "Soft Margins",
     "points": 0.5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert test_acc == [0.85, 0.9]\n",
         "hidden": false,
         "locked": false
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "Support Vectors": {
     "name": "Support Vectors",
     "points": 0.5,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert isinstance(plot_svm(20), sklearn.svm.SVC)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
