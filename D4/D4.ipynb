{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e0da39b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Initialize Otter\n",
    "import otter\n",
    "grader = otter.Notebook(\"D4.ipynb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b8bcfb",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-41bbfae80f7e2a64",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "# Evaluation Metrics & Model Selection (2 pts Possible)\n",
    "\n",
    "Your learning objective in lab 4 is to understand the theory & practice of model selection. This is the most important topic we will cover this quarter. Specifically you will\n",
    "- understand confusion matrices and classification metrics\n",
    "- implement cross validation\n",
    "- model selection w.r.t. lasso-regression\n",
    "- (optional/supplemental) review of the bias-variance tradeoff\n",
    "\n",
    "Fundamentally Model selection is about estimating how well your model will perform in the real world given some metric you care about. There are many model classes (linear regression, decision trees, neural networks, ...) and for every model class there are many possible choices, you the machine learning practitioner, can make (L1 vs. L2 regularization, the regularization strength, max tree depth, cost complexity,...). These choices often go beyond the scope of minimizing some error metric and require you to think about the nature of your problem, how it will be deployed, and the risks associated with different kinds of errors. Given the complex space of modeling decisions and the very real stakes of getting things wrong (false negative cancer test, false positive covid test, incorrectly denying someone a home loan, incorrectly approving someone of a home loan, etc...) it is paramount that you correctly estimate your models performance on (unseen) real world data and select your metrics according to real world stakes.\n",
    "\n",
    "Machine learning without an understanding of model selection and evaluation metrics can cause real world harms!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73543f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !!!IMPORTANT!!! uncomment if needed for module installation\n",
    "# Please comment out when you submit to gradescope\n",
    "\n",
    "# %pip install -q --force-reinstall git+https://github.com/COGS118A/quiz_module.git\n",
    "# %pip install -q otter-grader\n",
    "# %pip install -U scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13251183",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d34b9e9be345f42c",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.datasets import load_breast_cancer, load_diabetes\n",
    "from sklearn.metrics import f1_score, mean_squared_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from sklearn.metrics import zero_one_loss, accuracy_score, roc_auc_score, f1_score\n",
    "from sklearn.metrics import roc_curve, precision_recall_curve\n",
    "from sklearn.metrics import ConfusionMatrixDisplay, confusion_matrix\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "plt.rcParams['font.size'] = '16'\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "rng = np.random.RandomState(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437a7c6b",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-110d41f18bb5d1a6",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Setup for JupyterQuiz\n",
    "from quiz import display_quiz, record_quiz, check_quiz_answer, show_chosen_option\n",
    "from IPython.core.display import display, Javascript, HTML\n",
    "\n",
    "D4_path = \"https://raw.githubusercontent.com/COGS118A/DiscussionLabExercises/main/D4/\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6cafbb6",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2b6d061629054cd9",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 1. Metrics (1.1 pts)\n",
    "\n",
    "In this section of the lab, you will measure and analyze a binary classifier trained to detect breast cancer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f6e4f42",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-5cfe042015b14bd1",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Cancer Dataset\n",
    "\n",
    "The UCI ML Breast Cancer Wisconsin (Diagnostic) dataset is a binary classificiation task generated from microscopic images of 569 biopsy samples. The class of each sample is either malignant or benign cancer. The variables are statistical summaries of attributesmeasured from the cell nuclei of each image. For instance, attributes include the area of a cell nucleus, shape measurements (like concavity of nucleus), grayscale texture, and many others. Each such attribute is measured for each cell nucleus in the image. For each attribute three statistical summaries are calculated to create the features: the mean, standard error, and worst (largest) measurement made across all the cell nuclei in the image.\n",
    "\n",
    "For more details see https://scikit-learn.org/stable/datasets/toy_dataset.html#breast-cancer-dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f75bc12",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0ffcd435269e3a5d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_bcw  = load_breast_cancer()\n",
    "df_bcw = pd.DataFrame( data_bcw['data'], columns=data_bcw['feature_names'] )\n",
    "\n",
    "fig = plt.figure(figsize = (16,12))\n",
    "ax = fig.gca()\n",
    "df_bcw.hist(ax = ax);\n",
    "plt.tight_layout();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cc4d1a5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-539f954d49eef061",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Binary Classifier\n",
    "\n",
    "We will now train a binary classifier. Don't worry too much about the model details, your task will be to evaluate it. Our proceedure is as follows:\n",
    "\n",
    "1. create a model pipeline which (1) preprocesses the data and (2) runs it through logistic regression (binary classifier)\n",
    "2. split the data into train and test sets\n",
    "3. train the model\n",
    "\n",
    "If you are curious about a pipeline you can read more about them here on (sklearn's documentation page)[https://scikit-learn.org/stable/modules/generated/sklearn.pipeline.Pipeline.html]. In machine learning, there are transformations we might perform on data such as 1-hot-encoding, rescaling the data to have 0 mean and 1 variance. These transformations occur before we pass data to a model. A pipeline is a software abstraction that allows us to chain these transformations and a model together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51211d58",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-de10b9e8dca17e2d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# Think of this simply as a binary classifier. \n",
    "# Your job is to evaluate it.\n",
    "classifier_model = make_pipeline(\n",
    "    StandardScaler(), \n",
    "    LogisticRegression(random_state=0, penalty='l2')\n",
    ")\n",
    "\n",
    "# split our cancer dataset into a train and test\n",
    "X_train_bcw, X_test_bcw, y_train_bcw, y_test_bcw = train_test_split(\n",
    "    data_bcw['data'], data_bcw['target'], test_size=0.33, random_state=42)\n",
    "\n",
    "classifier_model.fit(X_train_bcw, y_train_bcw)\n",
    "\n",
    "y_pred_train_bcw = classifier_model.predict(X_train_bcw)\n",
    "y_pred_test_bcw = classifier_model.predict(X_test_bcw)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b2f5bbd",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-57b56014b20f74a8",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### Q: 1a confusion matrices (0.3 pts)\n",
    "\n",
    "A confusion matrix is a tool for measuring the occurrence of different kinds of correct and incorrect predictions in classification problems. In a binary classification problem (i.e. positive cancer prediction vs negative cancer prediction) your classifier may _incorrectly_ predict a `positive` cancer result in which case you have a `false positive` or your classifier may _incorrectly_ predict a `negative` cancer result in which case you have a `false negative`. Commonsense tells us the risks associated with these different errors are _very_ different: one increases the societal cost of healthcare the other _fails to detect cancer_ and may result in a preventable death. If you are familiar with statistics literature, you might know these as type I and type II errors. As a convention, the rows correspond to _actual_ classes and the columns correspond to _predicted_ classes.\n",
    "\n",
    "Note the for binary classification the confusion matrix shows you \n",
    "\n",
    "|                 |                |\n",
    "|-----------------|----------------|\n",
    "| True Negatives | False Positives |\n",
    "| False Negatives | True Positives |\n",
    "\n",
    "\n",
    "**Question** Given the below confusion matrix, please compute the recall, precision, sensitivity, and f1 metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58d111ff",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9d1f81e8ed7a2200",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test_bcw, y_pred_test_bcw)\n",
    "\n",
    "# the text version takes the true labels \n",
    "# and the predictions AND THE ORDER OF ARGUMENTS MATTERS!!\n",
    "# don't mess it up\n",
    "\n",
    "disp = ConfusionMatrixDisplay(cm);\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d833f3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "(true_negatives, false_positives,\n",
    " false_negatives, true_positives\n",
    ") = confusion_matrix(y_train_bcw, y_pred_train_bcw).ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27398275",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def recall(true_positives, false_positives, true_negatives, false_negatives):\n",
    "    \"\"\"computes recall\n",
    "    Arguments:\n",
    "        true_p# END QUESTIONositives: int\n",
    "            number of true positives\n",
    "        false_positives: int\n",
    "            number of false positives\n",
    "        true_negatives: int\n",
    "            number of true negatives\n",
    "        false_negatives: int\n",
    "            number of false negatives\n",
    "    Returns:\n",
    "        int\n",
    "        recall\n",
    "    \"\"\" \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91a2af4b",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"recall\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee9dac10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def precision(true_positives, false_positives, true_negatives, false_negatives):\n",
    "    \"\"\"computes precision\n",
    "    Arguments:\n",
    "        true_positives: int\n",
    "            number of true positives\n",
    "        false_positives: int\n",
    "            number of false positives\n",
    "        true_negatives: int\n",
    "            number of true negatives\n",
    "        false_negatives: int\n",
    "            number of false negatives\n",
    "    Returns:\n",
    "        int\n",
    "        precision\n",
    "    \"\"\" \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a224ae8f",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"precision\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a81957",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def harmonic_mean(a, b):\n",
    "    \"\"\"compute the harmonic mean...I think you will find this helpful!\n",
    "    \n",
    "    This mean is appropriate when you are taking the average of two rates, such as precision and recall.\n",
    "    \n",
    "    2*(a*b)/(a+b)\n",
    "    \n",
    "    https://en.wikipedia.org/wiki/Harmonic_mean\n",
    "    Arguments:\n",
    "        a: float\n",
    "            okay...this ought to be a rational number.\n",
    "        b: float\n",
    "            okay...this ought to be a rational number.\n",
    "    Returns:\n",
    "        float\n",
    "        harmonic_mean\n",
    "    \"\"\" \n",
    "    ...\n",
    "    \n",
    "def f1(true_positives, false_positives, true_negatives, false_negatives):\n",
    "    \"\"\"computes f1. hint hint: its the harmonic mean of precision and recall\n",
    "    Arguments:\n",
    "        true_positives: int\n",
    "            number of true positives\n",
    "        false_positives: int\n",
    "            number of false positives\n",
    "        true_negatives: int\n",
    "            number of true negatives\n",
    "        false_negatives: int\n",
    "            number of false negatives\n",
    "    Returns:\n",
    "        int\n",
    "        f1\n",
    "    \"\"\"\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c086de63",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "grader.check(\"f1_score\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7dae25c",
   "metadata": {},
   "source": [
    "### Metric Multiple Choice (0.5 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "59e51d0d",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-8d10cae73c91e29d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "HTML(display_quiz(f\"{D4_path}A.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8e238e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your choice\n",
    "with open(\"record.txt\", \"a+\") as r:\n",
    "    pass\n",
    "MCQ = [\"A1\", \"A2\"]\n",
    "for q in MCQ:\n",
    "    show_chosen_option(q)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "273fe876",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-21f1c35b68a4fbd2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 1b plot ROC curve & compute AUC (0.3 pts)\n",
    "\n",
    "The Reciever Operator Characteristic (ROC) characterizes binary classifiers by plotting the false positive rate against the false positive rate. How might a single trained model have multiple values for false positives and true positives you might ask? It turns out that vary what is called the decision threshold. In a decision tree for example, each node can report the probability of a class at a certain node, e.g., `P(cat|data) = 1/3` (read as 'probability of cat given our data'...or the probability our data point is a cat). You can set a decision threshold anywhere between 0 and 1. For our cat example, if the threshold for predicting `cat` is 1/4, then even though P(cat|data) < P(dog|data), we would predict `cat`. There are practical reasons for calibrating the decision threshold, e.g., if we wanted to build a database of every cat photo in the world we might accept a higher false positive rate (some dogs sneaking in) in exchange for maximizing the number of overall cats.\n",
    "\n",
    "For more information check out [this wonderful github page](https://github.com/dariyasydykova/open_projects/tree/master/ROC_animation).\n",
    "\n",
    "![img](https://github.com/dariyasydykova/open_projects/raw/master/ROC_animation/animations/cutoff.gif)\n",
    "\n",
    "NOTE: Cognitive science has used ROC curves to characterize the performance of human sensory and memory systems. ROC applies to more than just computer algorithms. In fact, it was first used to test radar operators in WWII.\n",
    "\n",
    "**Question** Given the classifier model `classifier_model` (defined in cells above) plot an ROC curve and compute the AUC.\n",
    "\n",
    "REMEMBER:\n",
    "$$FPR=\\frac{FP}{FP+TN}$$\n",
    "$$TPR=\\frac{TP}{TP+FN}$$\n",
    "\n",
    "AUC here is defined as the area under the ROC curve. (there are different ways to approximate this...think back to Reimann sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552bb61f",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-463e2e8eb826ed17",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# hint use classifier_model.decision_function(X_test_bcw) to get prediction scores\n",
    "scores = classifier_model.predict_proba(X_test_bcw)[:,1]\n",
    "fps = []\n",
    "tps = []\n",
    "for threshold in np.linspace(0,1,1000):\n",
    "    y_pred = scores > threshold\n",
    "    # Step 1: Compute the confusion matrix\n",
    "    # Step 2: Compute the TP and FP \n",
    "    # and append it to fps and tps\n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace90892",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def my_auc(fps, tps):\n",
    "    \"\"\"my_auc computes the AUC of ROC. Feel free to convert these to numpy arrays.\n",
    "    Arguments:\n",
    "        fps: List[float]\n",
    "            false positive rates\n",
    "        tps: List[float]\n",
    "            true positive rates\n",
    "    Returns:\n",
    "        float\n",
    "        the area under the ROC curve\n",
    "    \"\"\"    \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "165120dc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-dffbe088754bca60",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(8,8))\n",
    "ax.plot(fps, tps, linewidth=5, color='k')\n",
    "ax.plot(np.linspace(0,1, 20), np.linspace(0,1, 20), linewidth=5, color='b', linestyle='--')\n",
    "assert fps != [] and tps != []\n",
    "ax.fill_between(fps, tps, [0]*len(tps), color='grey', alpha=0.3)\n",
    "ax.set_xlabel(\"false positive rate\")\n",
    "ax.set_ylabel(\"true positive rate\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0066d150",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-0510fdda107ad8e0",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "## 2. Cross Validation (0.9 pts)\n",
    "\n",
    "Cross validation is a sampling based method for model selection. We first shuffle our data and then partition it into _k-folds_. Then we train $k$ models: each time we will use a fold to evaluate a model and train on the rest of the dataset. The primary goal of cross validation is to understand how well a model you train will generalize to (unseen) real world data. This also allows us to estimate the _variance_ or uncertainty of this prediction -- did each k-fold produce similar results or wildly different results? Think back to how the bias-variance relates.\n",
    "\n",
    "In this section you will complete your own cross validation algorithm.\n",
    "### **Q2.a (0.4 pts)**\n",
    "Implement cross validation to estimate the genearlization f1-score of a provided decision tree. Below you are provided a code skeleton you need to flesh out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6152acf9",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-9e88f8a57517ee5f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def permute(design_matrix, targets, rng):\n",
    "    \"\"\"Randomly permute (shuffle) your dataset!\n",
    "    Arguments:\n",
    "        design_matrix: np.ndarray\n",
    "            an N by M matrix where N is the number of examples and M is the number of features  \n",
    "        targets: np.ndarray\n",
    "            a column vector of dependent variables\n",
    "        rng:\n",
    "            a random number generator\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "        vectors sorted in unison, e.g., the rows are still paired\n",
    "    \"\"\"    \n",
    "    assert design_matrix.shape[0] == targets.shape[0]\n",
    "    permutation_indices = rng.permutation(len(design_matrix))\n",
    "    return design_matrix[permutation_indices, :], targets[permutation_indices]\n",
    "    \n",
    "def kfold_cross_validation(X, y, model, metric, folds=7):\n",
    "    \"\"\"kfold_cross_validation performs leave 1 out cross validation on some model to estimate some metric\n",
    "    \n",
    "    Arguments:\n",
    "        X: np.ndarray\n",
    "            an N by M matrix where N is the number of examples and M is the number of features  \n",
    "        targets: np.ndarray\n",
    "            a column vector of dependent variables\n",
    "        model:\n",
    "            an sklearn model\n",
    "        metric:\n",
    "            an sklean metric\n",
    "        folds: int\n",
    "            number of folds to perform\n",
    "    Returns:\n",
    "        Tuple[np.ndarray, np.ndarray]\n",
    "        vectors sorted in unison, e.g., the rows are still paired\n",
    "    \"\"\"   \n",
    "    rng = np.random.RandomState(0)\n",
    "    results = np.zeros(folds)\n",
    "    \n",
    "    #\n",
    "    X, y = permute(X, y, rng)\n",
    "\n",
    "    for k in range(folds):\n",
    "        N = (len(X) // folds)\n",
    "        start = k * N\n",
    "        end = start + N\n",
    "        if k == folds - 1 and N %2 == 1:\n",
    "            end = start + N + 1\n",
    "        # hint: np.delete might make your selection of training data much simpler\n",
    "        # hint: for training data you need to select everything not in the fold\n",
    "        # select a training X\n",
    "        training_X = ...\n",
    "        # select a training y\n",
    "        training_y = ...\n",
    "        # select a test X\n",
    "        test_X = ...\n",
    "        # select a test y\n",
    "        test_y = ...\n",
    "        fitted_model = model.fit(training_X, training_y) \n",
    "        pred_y = fitted_model.predict(test_X) \n",
    "        # metric\n",
    "        f1_score = ...\n",
    "        results[k] = f1_score\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a1d20c5",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-66c0a7c41ba50469",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "# here you will run your code. You don't need to edit anything here\n",
    "\n",
    "data  = load_breast_cancer()\n",
    "X = data[\"data\"]\n",
    "y = data[\"target\"]\n",
    "model = DecisionTreeClassifier(max_depth=3, random_state=0)\n",
    "results = kfold_cross_validation(X, y, model, f1_score, folds=7)\n",
    "mean_f1 = results.mean()\n",
    "std_f1 = results.std()\n",
    "print(f\"f1-score: {mean_f1:.3f}+-{std_f1:.3f}\")\n",
    "df = pd.DataFrame(results, columns=[\"f1-score\"])\n",
    "df.index.name = \"fold\"\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc33bb58",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-2568f45ade5d7461",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "How did the model perform across these k-folds? Did cross validation yield similar results? Should we be confident in our estimation of the models error?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8593a358",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-e9fe733d4dd13e35",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "### 3. L1 Loss Feature Selection (0.5 pts)\n",
    "\n",
    "Think back to regression, loss functions & regularization. If you only considered linear regression, you would already have quite a few modeling decisions to make: what loss function do I use? what regularization type do I use? If I do choose regularization, how strongly should I regularize my model?\n",
    "\n",
    "In this section, you will answer this for yourself. Specifically, you are going to see how the choice of regularization strength affects a model qualitatively (number of non-zero coefficients) and quantitatively (error on a heldout dataset).\n",
    "\n",
    "The model you will analyze is Lasso regression. This model uses a squared l2 error term and an l1 regularization term.\n",
    "\n",
    "$$||y - Xw||^2_2 + \\lambda ||W||_1$$\n",
    "\n",
    "In other words this is just ordinary least squares _but_ in addition to error we also punish models in which `sum(W)` is large. Think about what this does to the gradient (or see for yourself in the demo below)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bedeedb1",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-a751270919d47b0d",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "\n",
    "def plot_contour(f, x1bound, x2bound, resolution, ax):\n",
    "    x1range = np.linspace(x1bound[0], x1bound[1], resolution)\n",
    "    x2range = np.linspace(x2bound[0], x2bound[1], resolution)\n",
    "    xg, yg = np.meshgrid(x1range, x2range)\n",
    "    zg = np.zeros_like(xg)\n",
    "    for i,j in itertools.product(range(resolution), range(resolution)):\n",
    "        zg[i,j] = f([xg[i,j], yg[i,j]])\n",
    "    ax.contour(xg, yg, zg, 30)\n",
    "    \n",
    "    # Move left y-axis and bottim x-axis to centre, passing through (0,0)\n",
    "    ax.spines['left'].set_position('center')\n",
    "    ax.spines['bottom'].set_position('center')\n",
    "\n",
    "    # Eliminate upper and right axes\n",
    "    ax.spines['right'].set_color('none')\n",
    "    ax.spines['top'].set_color('none')\n",
    "    mini, minj = np.unravel_index(np.argmin(zg), zg.shape)\n",
    "    ax.scatter(xg[mini, minj], yg[mini, minj], marker='o', color='k', s=100)\n",
    "    return ax, (xg[mini, minj], yg[mini, minj])\n",
    "\n",
    "rng2 = np.random.RandomState(1234)\n",
    "N = 30\n",
    "xs = rng2.rand(N) * 10\n",
    "ys = -5*xs + 10 + 8*rng2.randn(N)\n",
    "\n",
    "REGULARIZATION_STRENGTHS = [10, 100, 500, 1000] # play with this parameter!!!\n",
    "\n",
    "def plot_gradient(REGULARIZATION_STRENGTH):\n",
    "    cost = lambda w: np.mean((w[0] + w[1]*xs - ys)**2)\n",
    "    l1_penalty = lambda w: np.sum(np.abs(w))\n",
    "    cost_plus_l1 = lambda w: cost(w) + REGULARIZATION_STRENGTH*l1_penalty(w)\n",
    "\n",
    "    # ignore this ugly plotting code\n",
    "    fig, axes = plt.subplots(ncols=3, nrows=1, constrained_layout=True, figsize=(15, 5))\n",
    "    _, minpnt1 = plot_contour(cost, [-20,20], [-20,20], 200, axes[0])\n",
    "    axes[0].set_title(\"∇l2(y - Wx)**2\")\n",
    "    plot_contour(l1_penalty, [-20,20], [-20,20], 200, axes[1])\n",
    "    axes[1].set_title(\"∇l1(W)\")\n",
    "    _, minpnt2 = plot_contour(cost_plus_l1, [-20,20], [-20,20], 200, axes[2])\n",
    "    axes[2].set_title(\"∇ l2(y - Wx)**2 + lambda*l1(W)\")\n",
    "    axes[2].scatter(*minpnt1, s=100, color='k', alpha=0.5, marker='o')\n",
    "    axes[2].arrow(*minpnt1, (minpnt2[0]-minpnt1[0])*.8, (minpnt2[1]-minpnt1[1])*.8, head_width = 1)\n",
    "    print((f\"REGULARIZATION_STRENGTH = {REGULARIZATION_STRENGTH}\"))\n",
    "    plt.show()\n",
    "    \n",
    "_ = [plot_gradient(strength) for strength in REGULARIZATION_STRENGTHS]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1f516cc",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-696b0b78ac6953ae",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "source": [
    "Anyhow, the choice of $\\lambda$ has significant consequences on your model. One way we can tune such hyperparameters is by systematically trying out various settings and comparing the results. We can do this with gridsearch where we try a range of values.\n",
    "\n",
    "**Question 3a (0.3 pts)** perform a grid search over possible regularization strengths to see which value produces a good model. You are provided some skeleton code you need to flesh out, in particular `number_of_non_zero_coefficients`. Also, while we typically write the regularization strength as $\\lambda$ (if your are curious why this is and like advanced math or physics...look up Lagrangians) but in sklearn's Lasso implementation, this parameter is called `alpha`.\n",
    "\n",
    "Hint: you should use `np.isclose` when comparing floats!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba20010",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-d123a4bfc1b68a84",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    }
   },
   "outputs": [],
   "source": [
    "data2 = load_diabetes()\n",
    "X2 = data2[\"data\"]\n",
    "y2 = data2[\"target\"]\n",
    "\n",
    "(X2_train, X2_test, \n",
    " y2_train, y2_test\n",
    ") = train_test_split(X2, y2, test_size=0.33, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e145ee3a",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-7cadb142ecdf645f",
     "locked": false,
     "schema_version": 3,
     "solution": true,
     "task": false
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_coefficients(lasso_pipeline_model):\n",
    "    \"\"\"get the vector of coefficients of the model\n",
    "    \n",
    "    Arguments:\n",
    "        lasso_pipeline_model: sklearn pipeline\n",
    "    Returns:\n",
    "        np.array\n",
    "        vector of weights (coefficients)\n",
    "    \"\"\"  \n",
    "    return lasso_pipeline_model.steps[1][1].coef_\n",
    "\n",
    "def number_of_non_zero_coefficients(lasso_pipeline_model):\n",
    "    \"\"\"compute mean squared error\n",
    "    \n",
    "    Arguments:\n",
    "        y_true: np.array\n",
    "            true response variable\n",
    "        y_pred: np.array\n",
    "            predicted response variable\n",
    "    Returns:\n",
    "        float\n",
    "        mean squared error\n",
    "    \"\"\"  \n",
    "    ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e15e246e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "alphas = np.linspace(0.01,2.5,20)\n",
    "non_zero_coefficients = []\n",
    "mean_squared_errors = []\n",
    "for alpha in alphas:\n",
    "    lasso_pipeline_model = make_pipeline(StandardScaler(), Lasso(random_state=0, alpha=alpha))\n",
    "    model = lasso_pipeline_model.fit(X2_train, y2_train)\n",
    "    non_zero_coefficients.append(\n",
    "        number_of_non_zero_coefficients(model)\n",
    "    )\n",
    "    mean_squared_errors.append(\n",
    "        r2_score(y2_test, model.predict(X2_test))\n",
    "    )\n",
    "    \n",
    "    \n",
    "fig, ax = plt.subplots(1, 1, figsize=(16,6))\n",
    "ax.plot(alphas, non_zero_coefficients, linewidth=5, color='k', label=\"number of non-zero coefficients\")\n",
    "ax2 = ax.twinx()\n",
    "ax2.plot(alphas, mean_squared_errors, linewidth=5, color='r', label=\"$r^2$\")\n",
    "ax.legend(loc='upper left')\n",
    "ax2.legend(loc='upper right');\n",
    "ax.set_xlabel(\"Regularization strength: call this alpha or $\\lambda$\")\n",
    "ax.set_ylabel(\"# of non-zero coefficients\")\n",
    "ax2.set_ylabel(\"$r^2$\");"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9823558c",
   "metadata": {},
   "source": [
    "### Regularization Quiz (0.2 pts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4835515",
   "metadata": {
    "nbgrader": {
     "grade": false,
     "grade_id": "cell-efedf59304b7b1f2",
     "locked": true,
     "schema_version": 3,
     "solution": false,
     "task": false
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "HTML(display_quiz(f\"{D4_path}B.txt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57127514",
   "metadata": {},
   "outputs": [],
   "source": [
    "# show your choice\n",
    "with open(\"record.txt\", \"a+\") as r:\n",
    "    pass\n",
    "MCQ = [\"B1\"]\n",
    "for q in MCQ:\n",
    "    show_chosen_option(q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac51c42",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%javascript\n",
    "IPython.notebook.save_notebook()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56cccb34",
   "metadata": {},
   "source": [
    "# Submission Guidelines\n",
    "Congratulations! You have completed this discussion lab.\n",
    "\n",
    "Have a look back over your answers, and also make sure to `Restart & Run All` from the kernel menu to double check that everything is working properly. This restarts everything and runs your code from top to bottom.\n",
    "\n",
    "When you are ready to submit your assignment, you can click `Validate` at the top. Note that in some assignments the code will take too long to run and validation may fail. Validation is just a final check that all the asserts are passing without failing.\n",
    "\n",
    "Once you're happy with your work, click the disk icon to save, and submit the zip file onto gradescope. **You MUST submit all the required component to receive credit.**\n",
    "\n",
    "Note that you can submit at any time, but **we grade your most recent submission**. This means that **if you submit an updated notebook after the submission deadline, it will be marked as late**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "370af71e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "otter": {
   "OK_FORMAT": true,
   "tests": {
    "cross_validation": {
     "name": "cross_validation",
     "points": 0.4,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "f1_score": {
     "name": "f1_score",
     "points": 0.1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(f1(1,1,1,1), 0.5)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "grid_search_regularization": {
     "name": "grid_search_regularization",
     "points": 0.3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "metric_multiple_choice": {
     "name": "metric_multiple_choice",
     "points": 0.5,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "precision": {
     "name": "precision",
     "points": 0.1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(precision(1,1,1,1), 0.5)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "recall": {
     "name": "recall",
     "points": 0.1,
     "suites": [
      {
       "cases": [
        {
         "code": ">>> assert np.isclose(recall(1,1,1,1), 0.5)\n",
         "hidden": false,
         "locked": false,
         "points": 0
        }
       ],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "regularization_quiz": {
     "name": "regularization_quiz",
     "points": 0.2,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    },
    "roc_auc_calculation": {
     "name": "roc_auc_calculation",
     "points": 0.3,
     "suites": [
      {
       "cases": [],
       "scored": true,
       "setup": "",
       "teardown": "",
       "type": "doctest"
      }
     ]
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
